{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13670934,"sourceType":"datasetVersion","datasetId":8680685}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:29:06.766829Z","iopub.execute_input":"2025-11-10T07:29:06.767125Z","iopub.status.idle":"2025-11-10T07:29:07.150895Z","shell.execute_reply.started":"2025-11-10T07:29:06.767103Z","shell.execute_reply":"2025-11-10T07:29:07.149791Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/acl-further-experiments/prob/prob_80/fake_news_80.csv\n/kaggle/input/acl-further-experiments/prob/prob_80/xl_sum_80.csv\n/kaggle/input/acl-further-experiments/prob/prob_20/xl_sum_20.csv\n/kaggle/input/acl-further-experiments/prob/prob_20/fake_news_20.csv\n/kaggle/input/acl-further-experiments/prob/prob_100/xl_sum_100.csv\n/kaggle/input/acl-further-experiments/prob/prob_100/fake_news_100.csv\n/kaggle/input/acl-further-experiments/prob/prob_40/xl_sum_40.csv\n/kaggle/input/acl-further-experiments/prob/prob_40/fake_news_40.csv\n/kaggle/input/acl-further-experiments/prob/prob_60/xl_sum_60.csv\n/kaggle/input/acl-further-experiments/prob/prob_60/fake_news_60.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install transformers -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:29:07.152206Z","iopub.execute_input":"2025-11-10T07:29:07.152695Z","iopub.status.idle":"2025-11-10T07:29:12.206591Z","shell.execute_reply.started":"2025-11-10T07:29:07.152662Z","shell.execute_reply":"2025-11-10T07:29:12.205272Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install git+https://github.com/csebuetnlp/normalizer -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:29:12.208859Z","iopub.execute_input":"2025-11-10T07:29:12.209212Z","iopub.status.idle":"2025-11-10T07:29:23.990563Z","shell.execute_reply.started":"2025-11-10T07:29:12.209177Z","shell.execute_reply":"2025-11-10T07:29:23.989620Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom normalizer import normalize\nfrom tqdm import tqdm\nimport re\nimport ast\nimport string","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:29:23.991783Z","iopub.execute_input":"2025-11-10T07:29:23.992109Z","iopub.status.idle":"2025-11-10T07:29:37.268587Z","shell.execute_reply.started":"2025-11-10T07:29:23.992075Z","shell.execute_reply":"2025-11-10T07:29:37.267656Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model_name = \"FabihaHaider/transliterated_nmt\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\nprint(torch_device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:29:37.270577Z","iopub.execute_input":"2025-11-10T07:29:37.271033Z","iopub.status.idle":"2025-11-10T07:30:10.239273Z","shell.execute_reply.started":"2025-11-10T07:29:37.271008Z","shell.execute_reply":"2025-11-10T07:30:10.238258Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c602ecf4b81549f09cc405087058df59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2100cadc11f143d7974228c13a7b918e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7365e7184f4c5494350569e134deb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa8ad9d43b8349c7905356850d976432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3bea623f8f8417383cc03e021e39e86"}},"metadata":{}},{"name":"stderr","text":"2025-11-10 07:29:42.532419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762759782.765483      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762759782.833677      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c174450a897e478eab7fe34094effa40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"714682ed00e045e3b43c079542c427d9"}},"metadata":{}},{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def predict_output(input_sentence):\n    input_sentence = normalize(input_sentence)\n    input_ids = tokenizer((input_sentence), return_tensors=\"pt\").input_ids.to(torch_device)\n    generated_tokens = model.generate(input_ids)\n    decoded_tokens = tokenizer.batch_decode(generated_tokens)[0]\n    decoded_tokens = normalize(decoded_tokens)\n\n    decoded_tokens = decoded_tokens.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n\n    return decoded_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:10.240377Z","iopub.execute_input":"2025-11-10T07:30:10.241029Z","iopub.status.idle":"2025-11-10T07:30:10.246120Z","shell.execute_reply.started":"2025-11-10T07:30:10.240995Z","shell.execute_reply":"2025-11-10T07:30:10.245249Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"predict_output(\"স্বাস্থ্যবান\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:10.247300Z","iopub.execute_input":"2025-11-10T07:30:10.247857Z","iopub.status.idle":"2025-11-10T07:30:11.228870Z","shell.execute_reply.started":"2025-11-10T07:30:10.247826Z","shell.execute_reply":"2025-11-10T07:30:11.227994Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'Hydroban'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def perturb_random_words(df, predict_fn, text_column, batch_size):\n    df = df.copy()\n    perturbed_texts = []\n    \n    # Process in batches with progress bar\n    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing word perturbations\"):\n        batch = df.iloc[i:i+batch_size]\n        \n        for _, row in batch.iterrows():\n            # Get word indices once\n            word_indices = eval(row['rand_word_idx']) if isinstance(row['rand_word_idx'], str) else row['rand_word_idx']\n            word_indices = [int(idx) for idx in word_indices]\n            word_indices.sort()\n            \n            # Split text into words only once\n            words = np.array(row[text_column].split())\n            \n            # Get valid indices\n            valid_indices = [idx for idx in word_indices if 0 <= idx < len(words)]\n            \n            if valid_indices:\n                # Get unique words to transliterate to avoid duplicate processing\n                unique_words = np.unique(words[valid_indices])\n                \n                # Create translation dictionary\n                translations = {word: predict_fn(word) for word in unique_words}\n                \n                # Apply translations efficiently\n                for idx in valid_indices:\n                    words[idx] = translations[words[idx]]\n            \n            perturbed_texts.append(' '.join(words))\n    \n    df['perturbed_text_words'] = perturbed_texts\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.229769Z","iopub.execute_input":"2025-11-10T07:30:11.230028Z","iopub.status.idle":"2025-11-10T07:30:11.238107Z","shell.execute_reply.started":"2025-11-10T07:30:11.229995Z","shell.execute_reply":"2025-11-10T07:30:11.237203Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"english_punctuation = set(string.punctuation)\nbangla_punctuation = set(\"।၊‐‑‒–—―‘’“”•…‧‰′″‹›‼‽⁄⁊⸘⸙⸚⸛⸜⸝⸞⸟⸠⸡⸢⸣⸤⸥⸮⸰⸱⸲⸳⸴⸵⸶⸷⸸⸹⸺⸻⸼⸽⸾⸿、。〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〽゠・︐︑︒︓︔︕︖︗︘︙︰︱︲︳︴︵︶︷︸︹︺︻︼︽︾︿﹀﹁﹂﹃﹄﹅﹆﹇﹈﹉﹊﹋﹌﹍﹎﹏﹐﹑﹒﹔﹕﹖﹗﹘﹙﹚﹛﹜﹝﹞﹟﹠﹡﹢﹣﹤﹥﹦﹨﹩﹪﹫！＂＃＄％＆＇（）＊＋，－．／：；＜＝＞？＠［＼］＾＿｀｛｜｝～｟｠｡｢｣､･\")\n\n# Combine all punctuation into a single set\nall_punctuation = english_punctuation.union(bangla_punctuation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.239017Z","iopub.execute_input":"2025-11-10T07:30:11.239354Z","iopub.status.idle":"2025-11-10T07:30:11.257964Z","shell.execute_reply.started":"2025-11-10T07:30:11.239325Z","shell.execute_reply":"2025-11-10T07:30:11.256960Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def remove_punctuation_elements(word_list):\n    # Define English and Bangla punctuation\n\n    # Filter out elements that are purely punctuation\n    cleaned_list = [word for word in word_list if word not in all_punctuation]\n\n    return cleaned_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.258979Z","iopub.execute_input":"2025-11-10T07:30:11.259291Z","iopub.status.idle":"2025-11-10T07:30:11.278077Z","shell.execute_reply.started":"2025-11-10T07:30:11.259266Z","shell.execute_reply":"2025-11-10T07:30:11.277085Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def replace_bengali_word(text, word_to_replace, replacement):\n    # Split into words while preserving spaces and punctuation\n    word_to_replace = normalize(word_to_replace)\n    # print(word_to_replace)\n    words = text.split(' ')\n    words = [normalize(word) for word in words]\n    # print(words)\n    # Replace exact word matches\n    return ' '.join(replacement if w == word_to_replace else w for w in words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.280316Z","iopub.execute_input":"2025-11-10T07:30:11.280899Z","iopub.status.idle":"2025-11-10T07:30:11.296714Z","shell.execute_reply.started":"2025-11-10T07:30:11.280869Z","shell.execute_reply":"2025-11-10T07:30:11.295775Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def perturb_salient_words(df, predict_fn, text_column, batch_size=32):\n    df = df.copy()\n    perturbed_texts = []\n    \n    # Process in batches with progress bar\n    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing salient word perturbations\"):\n        batch = df.iloc[i:i+batch_size]\n        \n        for _, row in batch.iterrows():\n            text = row[text_column]\n            for punct in all_punctuation:\n                text = text.replace(punct, f' {punct} ')\n            \n            # Remove extra spaces\n            text = ' '.join(text.split())\n            salient_words_str = row['salient_words']\n            \n            try:\n                salient_words = ast.literal_eval(salient_words_str)\n                salient_words = remove_punctuation_elements(salient_words)\n            except (ValueError, SyntaxError) as e:\n                print(f\"Error converting salient_words to list: {e}\")\n                salient_words = []\n                \n            perturbed_text = text\n            \n            if isinstance(salient_words, list):\n                for salient_word in salient_words:\n                    salient_word = normalize(salient_word)\n                    \n                    if salient_word:\n                        # print(salient_word)\n                        translation = predict_output(salient_word)\n                        # print(translation)\n                        perturbed_text = replace_bengali_word(perturbed_text, salient_word, translation)\n                        \n                    else:\n                        perturbed_text = perturbed_text\n            else:\n                print(\"not list\")\n                perturbed_text = perturbed_text\n            \n            perturbed_texts.append(perturbed_text)\n    \n    df['perturbed_text_salient'] = perturbed_texts\n    return df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.297652Z","iopub.execute_input":"2025-11-10T07:30:11.297917Z","iopub.status.idle":"2025-11-10T07:30:11.320246Z","shell.execute_reply.started":"2025-11-10T07:30:11.297894Z","shell.execute_reply":"2025-11-10T07:30:11.319255Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def perturb_random_sentences(df, predict_fn, text_column, batch_size=32):\n    df = df.copy()\n    perturbed_texts = []\n    \n    # Process in batches with progress bar\n    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing sentence perturbations\"):\n        batch = df.iloc[i:i+batch_size]\n        \n        for _, row in batch.iterrows():\n            # Get sentence indices once\n            sent_indices = eval(row['rand_sent_idx']) if isinstance(row['rand_sent_idx'], str) else row['rand_sent_idx']\n            sent_indices = [int(idx) for idx in sent_indices]\n            sent_indices.sort()\n            \n            # Split into sentences only once using regex for better performance\n            sentences = np.array([s.strip() for s in re.split('[।!?।৻॥]+', row[text_column]) if s.strip()])\n            \n            # Get valid indices\n            valid_indices = [idx for idx in sent_indices if 0 <= idx < len(sentences)]\n            \n            if valid_indices:\n                # Create a mapping of original sentences to their translations\n                sent_to_trans = {}\n                for idx in valid_indices:\n                    original_sent = sentences[idx]\n                    if original_sent not in sent_to_trans:\n                        translated = predict_fn(original_sent)\n                        sent_to_trans[original_sent] = translated\n                \n                # Replace sentences using the mapping\n                sentences_list = sentences.tolist()  # Convert to list for easier manipulation\n                for idx in valid_indices:\n                    original_sent = sentences_list[idx]\n                    sentences_list[idx] = sent_to_trans[original_sent]\n                \n                perturbed_texts.append('। '.join(sentences_list))\n            else:\n                perturbed_texts.append('। '.join(sentences))\n    \n    df['perturbed_text_sentences'] = perturbed_texts\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.321304Z","iopub.execute_input":"2025-11-10T07:30:11.321961Z","iopub.status.idle":"2025-11-10T07:30:11.342328Z","shell.execute_reply.started":"2025-11-10T07:30:11.321932Z","shell.execute_reply":"2025-11-10T07:30:11.341413Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"BATCH_SIZE = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.343266Z","iopub.execute_input":"2025-11-10T07:30:11.343625Z","iopub.status.idle":"2025-11-10T07:30:11.361536Z","shell.execute_reply.started":"2025-11-10T07:30:11.343563Z","shell.execute_reply":"2025-11-10T07:30:11.360455Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"filepaths = [\n    '/kaggle/input/acl-further-experiments/prob/prob_80/fake_news_80.csv',\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.362468Z","iopub.execute_input":"2025-11-10T07:30:11.363428Z","iopub.status.idle":"2025-11-10T07:30:11.376387Z","shell.execute_reply.started":"2025-11-10T07:30:11.363378Z","shell.execute_reply":"2025-11-10T07:30:11.375429Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"for filepath in filepaths:\n    df = pd.read_csv(filepath)\n    df0 = df[df['label'] == 0].sample(250, random_state=42)\n    df1 = df[df['label'] == 1].sample(250, random_state=42)\n    df = pd.concat([df0, df1]).sample(frac=1, random_state=42).reset_index(drop=True)\n    text_column = 'content'\n    df_perturbed = (df\n        .pipe(perturb_random_words, predict_output, text_column, BATCH_SIZE)\n        .pipe(perturb_random_sentences, predict_output, text_column, BATCH_SIZE)\n        .pipe(perturb_salient_words, predict_output, text_column, BATCH_SIZE)\n    )\n    \n    new_filepath = filepath.split('/')[-1]\n    print(new_filepath)\n    df_perturbed.to_csv(new_filepath, index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:30:11.377342Z","iopub.execute_input":"2025-11-10T07:30:11.378080Z","iopub.status.idle":"2025-11-10T07:51:57.602853Z","shell.execute_reply.started":"2025-11-10T07:30:11.378051Z","shell.execute_reply":"2025-11-10T07:51:57.601266Z"}},"outputs":[{"name":"stderr","text":"Processing word perturbations: 100%|██████████| 1/1 [08:14<00:00, 494.03s/it]\nProcessing sentence perturbations: 100%|██████████| 1/1 [06:28<00:00, 388.21s/it]\nProcessing salient word perturbations: 100%|██████████| 1/1 [07:02<00:00, 422.87s/it]","output_type":"stream"},{"name":"stdout","text":"xl_sum_80.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}